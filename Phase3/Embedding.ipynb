{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ae4bba",
   "metadata": {},
   "source": [
    "# <center> *Phase3(Part1):* **Embedding Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3498bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\AUT Courses\\Information Retrieval\\Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599a5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Phase1.PositionalBooleanSearch import preprocess\n",
    "from Phase2.VectorizedSearch import tfidf\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afbcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term:\n",
    "    def __init__(self, term, doc_id):\n",
    "        self.term = term\n",
    "        self.df = 1\n",
    "        self.tf_postings = {doc_id: 1}\n",
    "\n",
    "    def update_freq(self, doc_id):\n",
    "        if doc_id in self.tf_postings:\n",
    "            tf = self.tf_postings[doc_id]\n",
    "            self.tf_postings[doc_id] = tf + 1\n",
    "        else:\n",
    "            self.tf_postings[doc_id] = 1\n",
    "            self.df += 1\n",
    "\n",
    "    def tfidf_weight(self, doc_id, N):\n",
    "        tf = self.tf_postings[doc_id]\n",
    "        return tfidf(tf, self.df, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab5e5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDictionary:\n",
    "    def __init__(self, tokenized_docs):\n",
    "        self.N = len(tokenized_docs)\n",
    "        self.dictionary = self.build(tokenized_docs)\n",
    "\n",
    "    def build(self, tokenized_docs):\n",
    "        dictionary = {}\n",
    "\n",
    "        for doc_id in range(self.N):\n",
    "            document = tokenized_docs[doc_id]\n",
    "            for token in document:\n",
    "                if token in dictionary:\n",
    "                    term = dictionary[token]\n",
    "                    term.update_freq(doc_id)\n",
    "                    dictionary[token] = term\n",
    "                else:\n",
    "                    term = Term(token, doc_id)\n",
    "                    dictionary[token] = term\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        with open('..\\\\Phase3\\\\embedding_dictionary.pkl', 'wb') as output:\n",
    "            pickle.dump(self.dictionary, output)\n",
    "        return self.dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dictionary():\n",
    "        with open('..\\\\Phase3\\\\embedding_dictionary.pkl', 'rb') as input:\n",
    "            return pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa66779",
   "metadata": {},
   "source": [
    "## Tokenizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeed691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    contents = []\n",
    "    topics = []\n",
    "\n",
    "    df = pd.read_excel(\"..\\\\Phase3\\\\IR00_3_11k_News.xlsx\")\n",
    "    contents += df['content'].tolist()\n",
    "    topics += df['topic'].tolist()\n",
    "\n",
    "    df = pd.read_excel(\"..\\\\Phase3\\\\IR00_3_17k_News.xlsx\")\n",
    "    contents += df['content'].tolist()\n",
    "    topics += df['topic'].tolist()\n",
    "\n",
    "    df = pd.read_excel(\"..\\\\Phase3\\\\IR00_3_20k_News.xlsx\")\n",
    "    contents += df['content'].tolist()\n",
    "    topics += df['topic'].tolist()\n",
    "\n",
    "    return contents, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcbf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data():\n",
    "    documents, topics = read_documents()\n",
    "    print(len(documents))  # 50061\n",
    "    documents_tokens = preprocess(documents)\n",
    "    with open('..\\\\Phase3\\\\train_tokenized_documents.pkl', 'wb') as output:\n",
    "        pickle.dump(documents_tokens, output)\n",
    "    with open('..\\\\Phase3\\\\train_tags.pkl', 'wb') as output:\n",
    "        pickle.dump(topics, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722da15",
   "metadata": {},
   "source": [
    "## Building Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70f1ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_docs():\n",
    "    with open(\"..\\\\Phase3\\\\train_tokenized_documents.pkl\", 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485f14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    training_data = load_tokenized_docs()\n",
    "    cores = multiprocessing.cpu_count()\n",
    "\n",
    "    w2v_model = Word2Vec(min_count=1, window=5, vector_size=300, alpha=0.03, workers=cores - 1)\n",
    "    w2v_model.build_vocab(training_data)\n",
    "\n",
    "    w2v_model.train(training_data, total_examples=w2v_model.corpus_count, epochs=20)\n",
    "    w2v_model.save(\"w2v_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54bc31",
   "metadata": {},
   "source": [
    "## Convert Documents to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6d3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_docs_tfidf_dict():\n",
    "    tokenized_docs = load_tokenized_docs()\n",
    "    dictionary = EmbeddingDictionary(tokenized_docs).get_dictionary()\n",
    "    N = len(tokenized_docs)\n",
    "    docs_tfidf = []\n",
    "\n",
    "    for doc_id in range(N):\n",
    "        tokens = tokenized_docs[doc_id]\n",
    "        doc_tfidf = {}\n",
    "        for token in tokens:\n",
    "            term = dictionary[token]\n",
    "            doc_tfidf[token] = term.tfidf_weight(doc_id, N)\n",
    "        docs_tfidf.append(doc_tfidf)\n",
    "\n",
    "    return docs_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bacda5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_embedding():\n",
    "    docs_embedding = []\n",
    "    docs_tfidf = build_docs_tfidf_dict()\n",
    "    model = Word2Vec.load(\"w2v_model.model\")\n",
    "\n",
    "    for document in docs_tfidf:\n",
    "        doc_vector = np.zeros(300)\n",
    "        weights_sum = 0\n",
    "\n",
    "        for term, weight in document.items():\n",
    "            weights_sum += weight\n",
    "            try:\n",
    "                doc_vector += model.wv[term] * weight\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        if weights_sum == 0:\n",
    "            docs_embedding.append(doc_vector)\n",
    "            continue\n",
    "\n",
    "        docs_embedding.append(doc_vector / weights_sum)\n",
    "\n",
    "    return docs_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f7b885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_docs_vectors():\n",
    "    docs_vectors = documents_embedding()\n",
    "    with open('..\\\\Phase3\\\\train_docs_vectors.pkl', 'wb') as output:\n",
    "        pickle.dump(docs_vectors, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82fea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(query, model, dictionary):\n",
    "    query_terms = preprocess([query])[0]\n",
    "    query_vector = np.zeros(300)\n",
    "    N = len(dictionary)\n",
    "    weights_sum = 0\n",
    "\n",
    "    for qt in query_terms:\n",
    "        try:\n",
    "            qt_tf = query_terms.count(qt)\n",
    "            qt_df = dictionary[qt].df\n",
    "            weight = tfidf(qt_tf, qt_df, N)\n",
    "            query_vector += model.wv[qt] * weight\n",
    "        except KeyError:\n",
    "            # print(qt)\n",
    "            continue\n",
    "        weights_sum += weight\n",
    "\n",
    "    if weights_sum == 0:\n",
    "        return query_vector\n",
    "\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611ba48",
   "metadata": {},
   "source": [
    "Calling Funstions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72282c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_training_data()\n",
    "\n",
    "# build_model()\n",
    "\n",
    "# save_docs_vectors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
